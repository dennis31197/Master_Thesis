{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae239a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:00:31.403634Z",
     "start_time": "2021-11-05T08:00:23.607608Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e7c06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:00:31.497945Z",
     "start_time": "2021-11-05T08:00:31.485469Z"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "DURATION = 262144/SAMPLE_RATE  # in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2e1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run for GPU\n",
    "pyhsical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(pyhtiscal_devices[0],True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85977a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data not in memory\n",
    "dir = \"C:/Users/den/OneDrive - ORDIX AG/OneDrive/10_Studium/Masterarbeit/Python/testing\"\n",
    "file_name = \"waveform_data.npy\"\n",
    "\n",
    "\n",
    "file_path = os.path.join(dir, file_name)\n",
    "ar = np.load(file_path,mmap_mode=\"r\",allow_pickle=True)\n",
    "\n",
    "ar[0].shape\n",
    "#np.array(ar[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d8f389",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c3cb28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T08:10:36.694686Z",
     "start_time": "2021-11-09T08:10:36.688700Z"
    }
   },
   "outputs": [],
   "source": [
    "FILES_DIR = \"C:/Masterarbeit/music\"\n",
    "FILES_DIR_SAVE = \"C:/Masterarbeit/music_npy/sr_22050\"\n",
    "FILENAME_DIR = \"C:/Masterarbeit\"\n",
    "MONO = True   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88faf1de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T10:28:05.914541Z",
     "start_time": "2021-11-09T10:27:21.547204Z"
    }
   },
   "outputs": [],
   "source": [
    "signal = librosa.load(os.path.join(FILES_DIR, \"accidentally - onoyoko.mp3\"),\n",
    "                      sr=5512,\n",
    "                      #duration=30,\n",
    "                      mono=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dbb761",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T15:09:29.168696Z",
     "start_time": "2021-09-22T14:15:34.683759Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def Loader(file_path, sample_rate, duration, mono):\n",
    "\n",
    "    signal = librosa.load(file_path,\n",
    "                          sr=sample_rate,\n",
    "                          # duration=duration,\n",
    "                          mono=mono)[0]\n",
    "    return signal\n",
    "\n",
    "\n",
    "def split_signal(signal):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    amount_samples_in_segment = DURATION * SAMPLE_RATE\n",
    "    amount_splits = (len(signal))//amount_samples_in_segment\n",
    "    splitted_signal = []\n",
    "    for _ in range(amount_splits):\n",
    "        i = i + amount_samples_in_segment\n",
    "        if len(signal[j:i]) == amount_samples_in_segment:\n",
    "            splitted_signal.append(signal[j:i])\n",
    "        j = j + amount_samples_in_segment\n",
    "    return np.array(splitted_signal)\n",
    "\n",
    "\n",
    "signals = []\n",
    "for root, _, files in os.walk(FILES_DIR):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        signal = Loader(file_path, SAMPLE_RATE, DURATION, MONO)\n",
    "        splitted_signal = split_signal(signal)\n",
    "        i = 0\n",
    "        for signal in splitted_signal:\n",
    "            i = i + 1\n",
    "            filename = os.path.join(FILES_DIR_SAVE, file)\n",
    "            np.save(f\"{filename}_{i}\",np.array(signal))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3508c4db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T15:37:07.105645Z",
     "start_time": "2021-09-22T15:37:07.081709Z"
    }
   },
   "outputs": [],
   "source": [
    "# create file with all the filenames\n",
    "filenames=[]\n",
    "for root,_,files in os.walk(FILES_DIR_SAVE):\n",
    "    for file in files:\n",
    "        filenames.append(file)\n",
    "filename = os.path.join(\"C:/Masterarbeit\", \"filenames\")\n",
    "np.save(f\"{filename}.npy\",filenames) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf5eb86",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bb8ff8",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c40013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T15:30:18.571092Z",
     "start_time": "2021-10-08T15:30:18.545159Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_phaseshuffle(x, rad=2):\n",
    "    b, x_len, nch = list(x.shape)\n",
    "\n",
    "    phase = tf.random.uniform([], minval=-rad, maxval=rad + 1, dtype=tf.int32)\n",
    "    pad_l = tf.maximum(phase, 0)\n",
    "    pad_r = tf.maximum(-phase, 0)\n",
    "    phase_start = pad_r\n",
    "    x = tf.pad(x, [[0, 0], [pad_l, pad_r], [0, 0]], mode='reflect')\n",
    "\n",
    "    x = x[:, phase_start:phase_start+x_len]\n",
    "    tf.ensure_shape(x,[b, x_len, nch])\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123205b6",
   "metadata": {},
   "source": [
    "conv block für clean code später"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80208d0a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T15:30:18.664840Z",
     "start_time": "2021-10-08T15:30:18.636920Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_block(x, filters, kerne_size, strides, padding, n_layer,\n",
    "               kernel_initializer, kernel_constraint, use_bn=True,\n",
    "               use_phaseshuffle=True):\n",
    "    def phaseshuffle(x): return apply_phaseshuffle(x)\n",
    "    x = layers.Conv1D(filters=filters, kernel_size=kerne_size, strides=strides,\n",
    "                      padding=padding,\n",
    "                      kernel_initializer=kernel_initializer,\n",
    "                      kernel_constraint=kernel_constraint,\n",
    "                      name=f\"discr_conv_layer_{n_layer}\")(x)\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization(name=f\"discr_bn_layer_{n_layer}\")(x)\n",
    "    x = layers.LeakyReLU(alpha=0.2, name=f\"discr_LeakyReLU_layer_{n_layer}\")(x)\n",
    "    if use_phaseshuffle:\n",
    "        x = phaseshuffle(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb880f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T15:30:18.791500Z",
     "start_time": "2021-10-08T15:30:18.734652Z"
    }
   },
   "outputs": [],
   "source": [
    "def define_discriminator(input_shape=(262144, 1)):\n",
    "    def phaseshuffle(x): return apply_phaseshuffle(x)\n",
    "    init = keras.initializers.RandomNormal(stddev=0.02)\n",
    "    \n",
    "    inputs = keras.Input(shape=input_shape, name=\"discriminator_inputs\")\n",
    "    # downsample to 65536\n",
    "    x1 = layers.Conv1D(filters=64, kernel_size=25, strides=4,\n",
    "                       padding=\"same\",\n",
    "                       #kernel_initializer=init,\n",
    "                       name=\"discr_conv_layer_1\")(inputs)\n",
    "    x1 = layers.LeakyReLU(alpha=0.2, name=\"discr_LeakyReLU_layer_1\")(x1)\n",
    "    # downsample to 16384\n",
    "    x2 = layers.Conv1D(filters=64*2, kernel_size=25, strides=4,\n",
    "                       padding=\"same\",\n",
    "                       #kernel_initializer=init,\n",
    "                       name=\"discr_conv_layer_2\")(x1)\n",
    "    x2 = layers.LeakyReLU(alpha=0.2, name=\"discr_LeakyReLU_layer_2\")(x2)\n",
    "    x2 = phaseshuffle(x2)\n",
    "    # downsample to 4096\n",
    "    x3 = layers.Conv1D(filters=64*2, kernel_size=25, strides=4,\n",
    "                       padding=\"same\",\n",
    "                       #kernel_initializer=init,\n",
    "                       name=\"discr_conv_layer_3\")(x2)\n",
    "    x3 = layers.LeakyReLU(alpha=0.2, name=\"discr_LeakyReLU_layer_3\")(x3)\n",
    "    x3 = phaseshuffle(x3)\n",
    "    # downsample to 1024\n",
    "    x4 = layers.Conv1D(filters=64*4, kernel_size=25, strides=4,\n",
    "                       padding=\"same\",\n",
    "                       #kernel_initializer=init,\n",
    "                       name=\"discr_conv_layer_4\")(x3)\n",
    "    x4 = layers.LeakyReLU(alpha=0.2, name=\"discr_LeakyReLU_layer_4\")(x4)\n",
    "    x4 = phaseshuffle(x4)\n",
    "    # downsample to 256\n",
    "    x5 = layers.Conv1D(filters=64*8, kernel_size=25, strides=4,\n",
    "                       padding=\"same\",\n",
    "                       #kernel_initializer=init,\n",
    "                       name=\"discr_conv_layer_5\")(x4)\n",
    "    x5 = layers.LeakyReLU(alpha=0.2, name=\"discr_LeakyReLU_layer_5\")(x5)\n",
    "    x5 = phaseshuffle(x5)\n",
    "    # downsample to 64\n",
    "    x6 = layers.Conv1D(filters=64*16, kernel_size=25, strides=4,\n",
    "                       padding=\"same\",\n",
    "                       kernel_initializer=init,\n",
    "                       name=\"discr_conv_layer_6\")(x5)\n",
    "    x6 = layers.LeakyReLU(alpha=0.2, name=\"discr_LeakyReLU_layer_6\")(x6)\n",
    "    # downsample to 16\n",
    "    x7 = layers.Conv1D(filters=64*32, kernel_size=25, strides=4,\n",
    "                       padding=\"same\",\n",
    "                       #kernel_initializer=init,\n",
    "                       name=\"discr_conv_layer_7\")(x6)\n",
    "    x7 = layers.LeakyReLU(alpha=0.2, name=\"discr_LeakyReLU_layer_7\")(x7)\n",
    "    # classifier\n",
    "    x = layers.Flatten(name=\"discr_flatten\")(x7)\n",
    "    x = layers.Dropout(0.2, name=\"discr_dropout\")(x)\n",
    "    outputs = layers.Dense(units=1, activation=\"linear\",\n",
    "                           name=\"discr_output\")(x)\n",
    "    # model\n",
    "    discriminator = keras.Model(\n",
    "        inputs=inputs, outputs=outputs, name=\"discriminator\")\n",
    "\n",
    "\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4c07ae",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f291d85d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T15:30:18.903202Z",
     "start_time": "2021-10-08T15:30:18.885248Z"
    }
   },
   "outputs": [],
   "source": [
    "def upsample_block():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb66cd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T15:30:19.015898Z",
     "start_time": "2021-10-08T15:30:18.984982Z"
    }
   },
   "outputs": [],
   "source": [
    "def define_generator(latent_dim):\n",
    "    init = keras.initializers.RandomNormal(stddev=0.02)\n",
    "\n",
    "    inputs = keras.Input(shape=latent_dim, name=\"generator_inputs\")\n",
    "    # filters to begin with 2048 =32*64\n",
    "    n_nodes = 16*64*32\n",
    "    # upsample to 16\n",
    "    x = layers.Dense(units=n_nodes,\n",
    "                     #kernel_initializer=init,\n",
    "                     name=\"gener_dense_layer\")(inputs)\n",
    "    x = layers.Reshape(target_shape=(16, 64*32), name=\"gener_reshape\")(x)\n",
    "    x = layers.ReLU(name=\"gener_LeakyReLU_dense_layer\")(x)\n",
    "\n",
    "    # upsample to 64\n",
    "    x1 = layers.Conv1DTranspose(filters=64*16, kernel_size=25, strides=4,\n",
    "                                #kernel_initializer=init,\n",
    "                                padding=\"same\",\n",
    "                                name=\"gener_conv_layer_1\")(x)\n",
    "    x1 = layers.ReLU(name=\"gener_LeakyReLU_layer_1\")(x1)\n",
    "    # upsample to 256\n",
    "    x2 = layers.Conv1DTranspose(filters=64*8, kernel_size=25, strides=4,\n",
    "                                #kernel_initializer=init,\n",
    "                                padding=\"same\",\n",
    "                                name=\"gener_conv_layer_2\")(x1)\n",
    "    x2 = layers.ReLU(name=\"gener_LeakyReLU_layer_2\")(x2)\n",
    "    # upsample to 1024\n",
    "    x3 = layers.Conv1DTranspose(filters=64*4, kernel_size=25, strides=4,\n",
    "                                #kernel_initializer=init,\n",
    "                                padding=\"same\",\n",
    "                                name=\"gener_conv_layer_3\")(x2)\n",
    "    x3 = layers.LeakyReLU(alpha=0.2, name=\"gener_LeakyReLU_layer_3\")(x3)\n",
    "    # upsample to 4096\n",
    "    x4 = layers.Conv1DTranspose(filters=64*2, kernel_size=25, strides=4,\n",
    "                                #kernel_initializer=init,\n",
    "                                padding=\"same\",\n",
    "                                name=\"gener_conv_layer_4\")(x3)\n",
    "    x4 = layers.ReLU(name=\"gener_LeakyReLU_layer_4\")(x4)\n",
    "    # upsample to 16384\n",
    "    x5 = layers.Conv1DTranspose(filters=64*2, kernel_size=25, strides=4,\n",
    "                                #kernel_initializer=init,\n",
    "                                padding=\"same\",\n",
    "                                name=\"gener_conv_layer_5\")(x4)\n",
    "    x5 = layers.ReLU(name=\"gener_LeakyReLU_layer_5\")(x5)\n",
    "    # upsample to 65536\n",
    "    x6 = layers.Conv1DTranspose(filters=64, kernel_size=25, strides=4,\n",
    "                                #kernel_initializer=init,\n",
    "                                padding=\"same\",\n",
    "                                name=\"gener_conv_layer_6\")(x5)\n",
    "    x6 = layers.ReLU(name=\"gener_LeakyReLU_layer_6\")(x6)\n",
    "    # upsample to 262144 (~11s audio)\n",
    "    x7 = layers.Conv1DTranspose(filters=1, kernel_size=25, strides=4,\n",
    "                                #kernel_initializer=init,\n",
    "                                padding=\"same\",\n",
    "                                name=\"gener_conv_layer_7\")(x6)\n",
    "    # activation\n",
    "    outputs = keras.activations.tanh(x7)\n",
    "\n",
    "    # model\n",
    "    generator = keras.Model(inputs=inputs, outputs=outputs, name=\"generator\")\n",
    "\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5ef627",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed4fb4c",
   "metadata": {},
   "source": [
    "https://keras.io/examples/generative/wgan_gp/\n",
    "\n",
    "anmerkung: der hier verwendete algo weicht etwas von dem orginalen ab:\n",
    "https://jonathan-hui.medium.com/gan-wasserstein-gan-wgan-gp-6a1a2aa1b490\n",
    "bei dem originalen with die interpolation durch \n",
    "$\\hat{x} = t*\\tilde{x}+(1-t)$ mit $0<=t<=1 $\n",
    "erzeugt.\n",
    "Und hier durch hinufügen eines Wertes zu den real_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd1e7c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T15:30:19.150789Z",
     "start_time": "2021-10-08T15:30:19.098929Z"
    }
   },
   "outputs": [],
   "source": [
    "class WGAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim,\n",
    "                 discriminator_extra_steps=3, gp_weight=10\n",
    "                 ):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.gp_weight = gp_weight\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super(WGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_samples, fake_samples):\n",
    "\n",
    "        # get inerpolated sample, so a sample which lies between the real and the fake one\n",
    "        alpha = tf.random.normal(\n",
    "            shape=[batch_size, 1,1], mean=0.0, stddev=1.0)\n",
    "        diff = fake_samples - real_samples\n",
    "        interpolated = real_samples + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))\n",
    "        gp = tf.reduce_mean((norm-1.0)**2)\n",
    "        return gp\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(self, real_samples):\n",
    "        if isinstance(real_samples, tuple):\n",
    "            real_samples = real_samples[0]\n",
    "        batch_size = tf.shape(real_samples)[0]\n",
    "     \n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator.\n",
    "        for i in range(self.d_steps):\n",
    "            # get latent vector\n",
    "            random_latent_vectors = tf.random.normal(\n",
    "                shape=(batch_size, self.latent_dim)\n",
    "            )\n",
    "            with tf.GradientTape() as tape:\n",
    "                # generate fake sample form latent vector\n",
    "                fake_samples = self.generator(\n",
    "                    random_latent_vectors, training=True)\n",
    "                # get the logits for the fake samples\n",
    "                fake_logits = self.discriminator(fake_samples, training=True)\n",
    "                # get the logits for the reals samples\n",
    "                real_logits = self.discriminator(real_samples, training=True)\n",
    "                # calculate the discriminator loss using fake and real sample logits\n",
    "                d_cost = self.d_loss_fn(real_sample=real_logits,\n",
    "                                        fake_sample=fake_logits)\n",
    "                # calclate the gradient penalty\n",
    "                gp = self.gradient_penalty(\n",
    "                    batch_size, real_samples, fake_samples)\n",
    "                # add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "            # get the gradient w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(\n",
    "                d_loss, self.discriminator.trainable_variables)\n",
    "            # update the weights of the discriminator using the discrminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )  \n",
    "        # train the generator\n",
    "        # get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size,self.latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            #generate fake images using the generator\n",
    "            generated_samples = self.generator(random_latent_vectors,training=True)\n",
    "            # get the discriminator logits for fake images\n",
    "            gen_sample_logits = self.discriminator(generated_samples,training=True)\n",
    "            # calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_sample_logits)\n",
    "        # get the gradients w.r.t. the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # update the weights of the generator using the genrator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        return {\"d_loss\":d_loss, \"g_loss\":g_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4371f92",
   "metadata": {},
   "source": [
    "# Preprocessing / Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db56d75f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-22T13:45:02.084189Z",
     "start_time": "2021-09-22T13:45:02.076439Z"
    }
   },
   "outputs": [],
   "source": [
    "signals = np.load(\"./waveform_data.npy\",mmap_mode=\"r\",allow_pickle=True)\n",
    "signals = np.expand_dims(signals, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a9f258",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T15:30:19.245535Z",
     "start_time": "2021-10-08T15:30:19.230577Z"
    }
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(tf.keras.utils.Sequence) :\n",
    "  \n",
    "    def __init__(self, filenames, batch_size) :\n",
    "        self.filenames = filenames\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    \n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(self.filenames) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "    def __getitem__(self, idx) :\n",
    "        batch_x = self.filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        \n",
    "        sound_files = np.array([np.load(f\"{FILES_DIR_SAVE}/{file_name}\",allow_pickle=True)\n",
    "                         for file_name in batch_x])\n",
    "        sound_files = np.expand_dims(sound_files, axis=-1)\n",
    "        return sound_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4c6127",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af16c50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:05:49.273447Z",
     "start_time": "2021-11-05T08:05:49.255492Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_random_vector(n_samples,latent_dim):\n",
    "    return tf.random.uniform(shape=(n_samples,latent_dim),minval=-1, maxval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1750792f",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c711fbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T15:30:20.557036Z",
     "start_time": "2021-10-08T15:30:19.771128Z"
    }
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "global g_loss_history, d_loss_history\n",
    "g_loss_history, d_loss_history = [],[]\n",
    "\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self,latent_dim,sample_rate):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sample_rate = sample_rate\n",
    "        \n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        if epoch % 5 == 0:\n",
    "            random_latent_vector = tf.random.normal(shape=(1,self.latent_dim))\n",
    "            generated_sample = self.model.generator(random_latent_vector)\n",
    "            generated_sample = generated_sample.numpy().reshape((262144,))\n",
    "            sf.write(f\"./song_after_epoch_{epoch}.wav\", generated_sample, self.sample_rate)\n",
    "\n",
    "            model_name = f\"model_after_{epoch}_epochs.h5\"\n",
    "            self.model.generator.save(model_name)\n",
    "            \n",
    "            plt.plot(d_loss_history, label='d_loss')\n",
    "            plt.plot(g_loss_history, label='g_loss')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'loss_histroy_after_epoch_{epoch}.png')\n",
    "            plt.close()\n",
    "        \n",
    "    def on_train_batch_end(self,batch,logs=None):\n",
    "        d_loss_history.append(logs[\"d_loss\"])\n",
    "        g_loss_history.append(logs[\"g_loss\"])\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f0d0df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T15:30:27.511815Z",
     "start_time": "2021-10-08T15:30:23.409753Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# set number of epochs\n",
    "EPOCHS = 1\n",
    "# set latent dim\n",
    "latent_dim = 500\n",
    "# set batch_size\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# instantiate model\n",
    "g_model = define_generator(latent_dim)\n",
    "d_model = define_discriminator()\n",
    "\n",
    "# istantiate optimizers\n",
    "generator_optimizer = keras.optimizers.Adam(learning_rate=0.0002,\n",
    "                                            beta_1=0.5,\n",
    "                                            beta_2=0.9\n",
    "                                            )\n",
    "discriminator_optimizer = keras.optimizers.Adam(learning_rate=0.0002,\n",
    "                                               beta_1=0.5,\n",
    "                                               beta_2=0.9)\n",
    "# define losses\n",
    "# gp is added later to discriminator loss\n",
    "\n",
    "\n",
    "def discriminator_loss(real_sample, fake_sample):\n",
    "    real_loss = tf.reduce_mean(real_sample)\n",
    "    fake_loss = tf.reduce_mean(fake_sample)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "\n",
    "def generator_loss(fake_sample):\n",
    "    return -tf.reduce_mean(fake_sample)\n",
    "\n",
    "\n",
    "\n",
    "# instantiate the WGAN model\n",
    "wgan = WGAN(\n",
    "    discriminator=d_model,\n",
    "    generator=g_model,\n",
    "    latent_dim=latent_dim,\n",
    "    discriminator_extra_steps=5\n",
    ")\n",
    "wgan.compile(\n",
    "    d_optimizer=discriminator_optimizer,\n",
    "    g_optimizer=generator_optimizer,\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss\n",
    ")\n",
    "\n",
    "callbacks = [GANMonitor(latent_dim=latent_dim,sample_rate=SAMPLE_RATE)]\n",
    "\n",
    "training_data = BatchGenerator(np.load(f\"{FILENAME_DIR}/filenames.npy\",allow_pickle=True),BATCH_SIZE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992c00a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T15:37:24.509560Z",
     "start_time": "2021-10-08T15:30:37.431265Z"
    }
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "wgan.fit(training_data, batch_size=BATCH_SIZE,epochs=EPOCHS,callbacks=callbacks,verbose=1)\n",
    "end = time.time()\n",
    "process_time = end - start\n",
    "print(f'Process time for training: {round(process_time,3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69722ddf",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d830a8d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:07:41.288284Z",
     "start_time": "2021-11-05T08:07:40.030923Z"
    }
   },
   "outputs": [],
   "source": [
    "# example of loading the generator model and generating images\n",
    "from tensorflow.keras.models import load_model\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# load model\n",
    "model = load_model('./models/WGAN-GP_24_09/generator_after_20_epochs.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49080114",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:07:47.566320Z",
     "start_time": "2021-11-05T08:07:47.546373Z"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3db9aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:08:02.291925Z",
     "start_time": "2021-11-05T08:08:00.725261Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate images\n",
    "latent_points = get_random_vector(1,latent_dim=256)\n",
    "# generate images\n",
    "X = model.predict(latent_points)\n",
    "# plot the result\n",
    "#show_plot(X, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa4cb1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:08:04.303568Z",
     "start_time": "2021-11-05T08:08:04.296588Z"
    }
   },
   "outputs": [],
   "source": [
    "sound_file = X[0].reshape((262144,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137318e",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b841b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-05T08:08:36.206329Z",
     "start_time": "2021-11-05T08:08:36.185387Z"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import librosa.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f240eb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(model.get_weights())[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecd0c5e",
   "metadata": {},
   "source": [
    "## generated soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2f90c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-20T12:37:53.992292Z",
     "start_time": "2021-09-20T12:37:53.976344Z"
    }
   },
   "outputs": [],
   "source": [
    "signal1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8397d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T10:28:12.993941Z",
     "start_time": "2021-11-09T10:28:12.934720Z"
    }
   },
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 22050\n",
    "import soundfile as sf\n",
    "sf.write(\"./test.wav\", signal, SAMPLE_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8f7e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "wave = sound_file[0].reshape((262144,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4993be8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-09T10:28:08.767591Z",
     "start_time": "2021-11-09T10:28:06.262742Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,17))\n",
    "plt.subplot(3,1,1)\n",
    "librosa.display.waveplot(signal,alpha = 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.54499999999999,
   "position": {
    "height": "40px",
    "left": "1126.36px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
