{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1943b8dc",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e64167",
   "metadata": {},
   "source": [
    "This notebook first creates some custom layers that are used by the discriminator and the generator. Then functions to create the discriminator and the generator dependend on the level of the models and whether they should have the fade in paths or not. \n",
    "Both models are then passed to a WGAN_GP class where the training steps are defined. The training data is loaded in batches, so a Data Generator is defined as well as Callbacks to allow for monitoring during training.\n",
    "After instantiating the needed parameters the last section of code defines the training loop for training the whole algorithm with all levels. After every epoch of the full network the generator and the discriminator are saved to a monitoring folder. The training can therefore be interrupted and continued by loading the saved models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76922d85",
   "metadata": {},
   "source": [
    "# Imports and Gloabl Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990db230",
   "metadata": {},
   "source": [
    "* The library os is used get and crate the paths for the data. \n",
    "* The library time is used to measure the execution time for code snippets. That was used to debug and improve the code. For this notebook it is now only used to measure the execution time one one batch.\n",
    "* The library numpy is used to create and load the training data and to execute mathmatical operations. \\\n",
    "* The library tensorflow is used create, save and load models and layers and to create Callbacks and the data generator. \\\n",
    "* The library gc is used to execute python gargabe collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae239a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T16:35:08.252116Z",
     "start_time": "2021-11-16T16:35:04.938605Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import load_model\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87e7c06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T09:35:18.939927Z",
     "start_time": "2021-11-16T09:35:18.928780Z"
    }
   },
   "outputs": [],
   "source": [
    "FINISH_SAMPLE_RATE = 22050  # sample rate of the final level\n",
    "DURATION = 262144/FINISH_SAMPLE_RATE  # in seconds; 262144 is a power of 2 and the number of samples of the final level\n",
    "FILENAME_DIR = \"C:/Masterarbeit\"  # where the file with the names of the audio data is\n",
    "TRAINING_DATA_BASE_DIR = \"C:/Masterarbeit/music_npy\" # where the .npy files are with the numeric wave data\n",
    "MONITORING_DIR = \"./Monitoring2\" # to save the monitoring data while training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf5eb86",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ac6d9e",
   "metadata": {},
   "source": [
    "## Custom Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c0fa02",
   "metadata": {},
   "source": [
    "### Weighted Sum\n",
    "\n",
    "The code for the weighted sum layer was taken from following tutorial and was verfied by double checking the code of the original growing gan paper: \\\n",
    "Tutorial: https://machinelearningmastery.com/how-to-implement-progressive-growing-gan-models-in-keras/ \\\n",
    "Original paper: https://github.com/tkarras/progressive_growing_of_gans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1016ef2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T09:35:19.005385Z",
     "start_time": "2021-11-16T09:35:18.992225Z"
    }
   },
   "outputs": [],
   "source": [
    "class WeightedSum(keras.layers.Add):\n",
    "    '''\n",
    "    inherits from keras Add layer\n",
    "    takes to inputs multiplies them by a factor and adds them up\n",
    "    outputs the weighted sum \n",
    "    '''\n",
    "    # init with default value\n",
    "    def __init__(self, alpha=0.0, **kwargs):\n",
    "        super(WeightedSum, self).__init__(**kwargs)   # initializes the parent class constructor\n",
    "        self.alpha = keras.backend.variable(alpha, name='ws_alpha') # initializes the class variales alpha\n",
    "\n",
    "    # output a weighted sum of inputs\n",
    "    def _merge_function(self, inputs):\n",
    "        # only supports a weighted sum of two inputs\n",
    "        assert (len(inputs) == 2) # make sure it is only two inputs\n",
    "        # ((1-a) * input1) + (a * input2)\n",
    "        output = ((1.0 - self.alpha) * inputs[0]) + (self.alpha * inputs[1]) \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab5c2dc",
   "metadata": {},
   "source": [
    "### Mini Batch StDev\n",
    "The code for the weighted sum layer was taken from following tutorial and was verfied by double checking the code of the original growing gan paper: \\\n",
    "Tutorial: https://machinelearningmastery.com/how-to-train-a-progressive-growing-gan-in-keras-for-synthesizing-faces/ \\\n",
    "Original paper: https://github.com/tkarras/progressive_growing_of_gans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4400b7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T09:35:19.080918Z",
     "start_time": "2021-11-16T09:35:19.067090Z"
    }
   },
   "outputs": [],
   "source": [
    "# mini-batch standard deviation layer\n",
    "class MinibatchStdev(keras.layers.Layer):\n",
    "    '''\n",
    "    inheris from keras layer\n",
    "    caculates the standardeviation for every position for every feature map across a minibatch\n",
    "    and consolidates the information to a single value by taking the average\n",
    "    this average value is replicated and added to the data as a new feature map\n",
    "    '''\n",
    "    def __init__(self, **kwargs):\n",
    "        super(MinibatchStdev, self).__init__(**kwargs)  # initializes the parent class constructor\n",
    "\n",
    "    # perform the operation\n",
    "    def call(self, inputs):\n",
    "\n",
    "        mean = keras.backend.mean(inputs, axis=0, keepdims=True)  # calculate the mean value for each position across feature maps\n",
    "        squ_diffs = keras.backend.square(inputs - mean)  # calculate the squared differences between signal values and mean\n",
    "        mean_sq_diff = keras.backend.mean(squ_diffs, axis=0, keepdims=True) # calculate the average of the squared differences (variance)\n",
    "        mean_sq_diff += 1e-8  # add a small value to avoid a blow-up when we calculate stdev\n",
    "        stdev = keras.backend.sqrt(mean_sq_diff) # square root of the variance (stdev)\n",
    "        mean_pix = keras.backend.mean(stdev, keepdims=True) # calculate the mean standard deviation across each position\n",
    "        # scale this up to be the size of one input feature map for each sample\n",
    "        shape = keras.backend.shape(inputs)\n",
    "        output = keras.backend.tile(mean_pix, (shape[0], shape[1], 1))\n",
    "        combined = keras.backend.concatenate([inputs, output], axis=-1) # concatenate with the output\n",
    "        return combined\n",
    "\n",
    "    # define the output shape of the layer\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        input_shape = list(input_shape) # create a copy of the input shape as a list\n",
    "        input_shape[-1] += 1 # add one to the channel dimension \n",
    "        return tuple(input_shape) # convert list to a tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb132623",
   "metadata": {},
   "source": [
    "### Pixel Normalization\n",
    "The code for the weighted sum layer was taken from following tutorial and was verfied by double checking the code of the original growing gan paper: \\\n",
    "Tutorial: https://machinelearningmastery.com/how-to-train-a-progressive-growing-gan-in-keras-for-synthesizing-faces/ \\\n",
    "Original paper: https://github.com/tkarras/progressive_growing_of_gans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d6b11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T09:35:19.152042Z",
     "start_time": "2021-11-16T09:35:19.145852Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class PixelNormalization(keras.layers.Layer):\n",
    "    '''\n",
    "    inherits from keras layer\n",
    "    scales the feature vector for every position to unit length\n",
    "    '''\n",
    "    # initialize the layer\n",
    "    def __init__(self, **kwargs):\n",
    "        super(PixelNormalization, self).__init__(**kwargs) # initializes the parent class constructor\n",
    "\n",
    "    # perform the operation\n",
    "    def call(self, inputs):\n",
    "        values = inputs**2.0 # calculate square signal values     \n",
    "        mean_values = keras.backend.mean(values, axis=-1, keepdims=True) # calculate the mean signal values      \n",
    "        mean_values += 1.0e-8 # ensure the mean is not zero\n",
    "        l2 = keras.backend.sqrt(mean_values) # calculate the sqrt of the mean squared value (L2 norm)\n",
    "        normalized = inputs / l2 # normalize values by the l2 norm\n",
    "        return normalized\n",
    "\n",
    "    # define the output shape of the layer\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aff5fde",
   "metadata": {},
   "source": [
    "### Equalized Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684cdae3",
   "metadata": {},
   "source": [
    "The code defined a new type of layer called Conv1DEQ implementing the equalized learning rate from He (2015). The code was inspired but not copied from the orignal growing gan paper: \\\n",
    "Equalized learning rate defined here: https://arxiv.org/pdf/1502.01852.pdf p.3 \\\n",
    "Code from original paper: https://github.com/tkarras/progressive_growing_of_gans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c401660e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T09:35:19.226076Z",
     "start_time": "2021-11-16T09:35:19.211716Z"
    }
   },
   "outputs": [],
   "source": [
    "# normalisiert die gewichte während der Laufzeit pro Layer mit einer Layer eigenen konstante\n",
    "\n",
    "class Conv1DEQ(keras.layers.Conv1D):\n",
    "    \"\"\"\n",
    "    inherits conv1d from keras layers \n",
    "    extends this by applying scaling of the weights with He's per-layer constant \n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(kernel_initializer=keras.initializers.RandomNormal(stddev=1), **kwargs) # initializes the parent class constructor\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape) # build the parant class\n",
    "        n = np.product([self.kernel_size[0],input_shape[-1]]) # multiply the kernel length with number of feature maps\n",
    "        self.c = np.sqrt(2/n) # He initialization constant\n",
    "\n",
    "    def call(self, inputs):\n",
    "        outputs = tf.nn.conv1d( ## apply tensorflow conv1d layer\n",
    "            inputs,\n",
    "            self.kernel/self.c, # scale kernel\n",
    "            stride=self.strides,\n",
    "            padding=\"SAME\")\n",
    "\n",
    "        if self.use_bias: # add bias if there is one\n",
    "            outputs = tf.nn.bias_add(\n",
    "                outputs,\n",
    "                self.bias)\n",
    "\n",
    "        if self.activation is not None: # use activation if there is one\n",
    "            return self.activation(outputs) \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b15e821",
   "metadata": {},
   "source": [
    "The Conv1DEQ_load class is basicall the same layer as the one above. The only difference is that this one does not take a kernel initializer in the constructur. This is needed to avoid an error when loading the model because the weights of the loaded model are already defined so no initialization is neededn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c10687",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T09:35:19.291145Z",
     "start_time": "2021-11-16T09:35:19.284226Z"
    }
   },
   "outputs": [],
   "source": [
    "class Conv1DEQ_load(keras.layers.Conv1D):\n",
    "    \"\"\"\n",
    "    inherits conv1d from keras layers \n",
    "    extends this by applying scaling of the weights with He's per-layer constant \n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs) # initialize parent constructor without weight initialization\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        # The number of inputs\n",
    "        n = np.product([self.kernel_size[0],input_shape[-1]])\n",
    "        # He initialisation constant\n",
    "        self.c = np.sqrt(2/n)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        outputs = tf.nn.conv1d(\n",
    "            inputs,\n",
    "            self.kernel*self.c, # scale kernel\n",
    "            stride=self.strides,\n",
    "            padding=\"SAME\")\n",
    "\n",
    "        if self.use_bias:\n",
    "            outputs = tf.nn.bias_add(\n",
    "                outputs,\n",
    "                self.bias)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bb8ff8",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570717d8",
   "metadata": {},
   "source": [
    "This part of the code consists of two functions to create the discriminator model. The function create_discriminator is called later to define the discriminator for a given level and with the fade in path or without and calls the create_discriminator_block function. Level one corresponds to n_blocks = 0 because no additional blocks are needed. If the fade in parameter is set to True the split path as explained in the thesis is added to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f95a26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-16T09:35:19.390742Z",
     "start_time": "2021-11-16T09:35:19.359399Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of filters used for the different levels\n",
    "n_filters = {0:256,1:256,2:128,3:64,4:64,5:32,6:32,7:16,8:16}\n",
    "\n",
    "def create_discriminator_block(x,block_n):\n",
    "    '''\n",
    "    creates a new convolutional block consisting of two conv layers with equalized learning rate and Leaky Relu activation function \n",
    "    and an average pooling layer at the end\n",
    "    x: keras layer\n",
    "    block_n: number of current block that is added\n",
    "    '''\n",
    "    x = Conv1DEQ(filters=n_filters[block_n],kernel_size=25, padding='same' ,name=f\"block{block_n}_Conv1D_1\")(x)\n",
    "    x = keras.layers.LeakyReLU(alpha=0.2,name=f\"block{block_n}_LeakyRelu_1\")(x)\n",
    "    x = Conv1DEQ(filters=n_filters[block_n-1], kernel_size=25, padding='same',name=f\"block{block_n}_Conv1D_2\")(x)\n",
    "    x = keras.layers.LeakyReLU(alpha=0.2,name=f\"block{block_n}_LeakyRelu_2\")(x)\n",
    "    x = keras.layers.AveragePooling1D(pool_size=2,strides=2,\n",
    "                                      padding=\"same\",name=f\"block{block_n}_AveragePooling1D\")(x) \n",
    "    return x\n",
    "\n",
    "# erstellt den discriminator mit der Anzahl der Blöcke die übergeben wird\n",
    "# fade_in gibt ob es mit oder ohne Übergangsphase ist\n",
    "def create_discriminator(n_blocks,fade_in=False):\n",
    "    '''\n",
    "    creats the discriminator network as either a fade in network or a full level network with the number of conv blocks passed\n",
    "    n_blocks: basically indicates the level of the discriminator with 0 being the first level and 8 being the final\n",
    "    fade_in: indicate if its a fade in network or a full network (True/False)\n",
    "    '''\n",
    "    if n_blocks==0: fade_in=False # for 0 blocks fade_in alwayse False (better: do error handling)\n",
    "    input_shape = (1024*2**n_blocks,1) # input_shape of the discriminator depends on number of blocks added\n",
    "    inputs = keras.layers.Input(shape=input_shape,name=\"discriminator_inputs\") # define keras input\n",
    "    x = Conv1DEQ(filters=n_filters[n_blocks], kernel_size=1, padding='same', name=\"input_Conv1D_1\")(inputs) # input Conv layer with kernel size 1\n",
    "    x = keras.layers.LeakyReLU(alpha=0.2,name=f\"input_LeakyRelu_1\")(x) # ReLU activation \n",
    "    \n",
    "    if fade_in: # create fade in network if fade_in is set to True\n",
    "        new_block = create_discriminator_block(x,n_blocks) # create the new Conv block that is faded in smoothly\n",
    "        \n",
    "        downsample = keras.layers.AveragePooling1D(pool_size=2,strides=2,  # downsample the input directly before processing\n",
    "                                                   padding=\"same\",\n",
    "                                                   name=f\"Downsample_AveragePooling1D_fade_in_model\")(inputs)\n",
    "        x = Conv1DEQ(filters=n_filters[n_blocks-1], kernel_size=1, padding='same', name=f\"block{n_blocks-1}_input_Conv1D_1\")(downsample) # input conv layer\n",
    "        old_block = keras.layers.LeakyReLU(alpha=0.2,name=f\"block{n_blocks-1}_input_LeakyRelu_1\")(x) # downsampled input from the old block\n",
    "        \n",
    "        x = WeightedSum(name=\"WeightedSum_Layer\")([old_block, new_block]) # weighted sum of old and new block\n",
    "        \n",
    "        for block_n in range(n_blocks-1,0,-1):  # build the rest of the network \n",
    "            x = create_discriminator_block(x,block_n)             \n",
    "        \n",
    "    else: # create full network if fade_in is set to False\n",
    "        for block_n in range(n_blocks,0,-1): # directly build all blocks without a split path\n",
    "            x = create_discriminator_block(x,block_n)\n",
    "        \n",
    "    x = MinibatchStdev(name=\"MinibatchStdev\")(x) # add mini batch stdev as a new feature map\n",
    "    # add last conv block which ist the block form level 1\n",
    "    x = Conv1DEQ(filters=256,kernel_size=25 , padding='same', name=\"block_0_Conv1D_1\")(x) \n",
    "    x = keras.layers.LeakyReLU(alpha=0.2,name=f\"block_0_LeakyRelu_1\")(x)\n",
    "    x = Conv1DEQ(filters=256, kernel_size=25, padding='same', name=\"block_0_Conv1D_2\")(x)\n",
    "    x = keras.layers.LeakyReLU(alpha=0.2,name=f\"block_0_LeakyRelu_2\")(x)\n",
    "    \n",
    "    x = keras.layers.Flatten(name=\"Flatten_Layer\")(x) # Flatten the output of the conv block\n",
    "    out_class = keras.layers.Dense(1,name=\"discriminator_outputs\")(x) # consolidate all information to a single number, no activation = linear activation\n",
    "\n",
    "    model = keras.models.Model(inputs, out_class,name=\"discriminator_model\") # create the keras model\n",
    "    return model # return the model\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a57a606",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451f3649",
   "metadata": {},
   "source": [
    "This part of the code consists of two functions to create the generator model. The function create_generator is called later to define the generator for a given level and with the fade in path or without and calls the create_generator_block function. The latent dimension is set later in the notebook.\n",
    "Level one corresponds to n_blocks = 0 because no additional blocks are needed. If the fade in parameter is set to True the split path as explained in the thesis is added to the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2496cd64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T09:46:50.369370Z",
     "start_time": "2021-11-04T09:46:50.348912Z"
    }
   },
   "outputs": [],
   "source": [
    "# number of filters used for the different levels\n",
    "n_filters = {0:256,1:256,2:128,3:64,4:64,5:32,6:32,7:16,8:16}\n",
    "\n",
    "def create_generator_block(x,block_n):\n",
    "    '''\n",
    "    creates a new convolutional block consisting of and upsampling layer followed by\n",
    "    two convolutional layer with equalized learning rate, pixel normalization \n",
    "    and a leaky ReLu activation function\n",
    "    x: keras Layer\n",
    "    block_n: number of current block that is added\n",
    "    '''\n",
    "    upsampling = keras.layers.UpSampling1D(size=2,name=f\"block{block_n}_Upsampling\")(x)\n",
    "    x = Conv1DEQ(filters=n_filters[block_n], kernel_size=25, padding='same',name=f\"block{block_n}_Conv1D_1\")(upsampling)\n",
    "    x = PixelNormalization(name=f\"block{block_n}_PixNorm_1\")(x)\n",
    "    x = keras.layers.LeakyReLU(alpha=0.2,name=f\"block{block_n}_LeakyRelu_1\")(x)\n",
    "    x = Conv1DEQ(filters=n_filters[block_n], kernel_size=25, padding='same',name=f\"block{block_n}_Conv1D_2\")(x)\n",
    "    x = PixelNormalization(name=f\"block{block_n}_PixNorm_2\")(x)\n",
    "    x = keras.layers.LeakyReLU(alpha=0.2,name=f\"block{block_n}_LeakyRelu_2\")(x)\n",
    "    return x\n",
    "    \n",
    "    \n",
    "def create_generator(latent_dim,n_blocks,fade_in=False,in_dim=1024):\n",
    "    '''\n",
    "    creats the generator network as either a fade in network or a full level network with the number of conv blocks passed\n",
    "    latent_dim: dimension of the latent vector that is fed as an input to the generator\n",
    "    n_blocks: basically indicates the level of the discriminator with 0 being the first level and 8 being the final\n",
    "    fade_in: indicate if its a fade in network or a full network (True/False)    \n",
    "    in_dim: starting dimension of the network, for out case alwayse 1024 \n",
    "    '''\n",
    "    if n_blocks==0: fade_in=False  # for 0 blocks fade_in alwayse False (better: do error handling)\n",
    "    init = keras.initializers.RandomNormal(stddev=1) # weight initialization for the dense layer\n",
    "    inputs = keras.layers.Input(shape=(latent_dim,),name=\"generator_inputs\") # keras input\n",
    "    x = keras.layers.Dense(4 * in_dim, kernel_initializer=init,name=\"Dense_Layer\")(inputs)  # scale the input to the starting dimension with four feature maps\n",
    "    x = keras.layers.Reshape((in_dim, 4),name=\"Reshape_Layer\")(x) # Reshape to the starting dimension with four feature maps\n",
    "    # first convolutional block which is the same for all levels\n",
    "    x = Conv1DEQ(filters=256, kernel_size=25, padding='same', name=\"block0_Conv1D_1\")(x) \n",
    "    x = PixelNormalization(name=\"block0_PixNorm_1\")(x)\n",
    "    x = keras.layers.LeakyReLU(alpha=0.2,name=\"block0_LeackRelu_1\")(x)\n",
    "    x = Conv1DEQ(filters=256, kernel_size=25, padding='same', name=\"block0_Conv1D_2\")(x)\n",
    "    x = PixelNormalization(name=\"block0_PixNorm_2\")(x)\n",
    "    x = keras.layers.LeakyReLU(alpha=0.2,name=\"block0_LeackRelu_2\")(x)   \n",
    "    \n",
    "    if fade_in: # create fade in network if fade_in is set to True\n",
    "        for block_n in range(n_blocks-1): # for fade in = True add all blocks expcept for the last one because the path is split\n",
    "            x = create_generator_block(x,block_n+1)    \n",
    "\n",
    " \n",
    "        upsampling = keras.layers.UpSampling1D(size=2,name=f\"fade_in_Upsampling\")(x) # upsample the output of the last conv block, which is the output of the last level\n",
    "        output_old = Conv1DEQ(filters=1, kernel_size=1, padding='same',activation=\"tanh\", # add output conv layer\n",
    "                             name=\"generator_output_old\")(upsampling) \n",
    "        x = create_generator_block(x,n_blocks) # add the new conv block\n",
    "        output_new = Conv1DEQ(filters=1, kernel_size=1, padding='same',activation=\"tanh\", # add the output conv layer for the new conv block\n",
    "                             name=\"generator_output_new\")(x)  \n",
    "        outputs = WeightedSum(name=\"weighted_sum_layer\")([output_old, output_new]) # build the weighed sum of the ols and new block           \n",
    "        \n",
    "    else: # create full network if fade_in is set to False\n",
    "        for block_n in range(n_blocks): # add all blocks without a split path\n",
    "            x = create_generator_block(x,block_n+1)    \n",
    "\n",
    "        outputs = Conv1DEQ(filters=1, kernel_size=1, padding='same',activation=\"tanh\", # add the output layer\n",
    "                                 name=\"generator_outptus\")(x)\n",
    "    model = keras.models.Model(inputs, outputs,name=\"generator_model_full\") # build the keras model\n",
    "    return model # return the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29832bd6",
   "metadata": {},
   "source": [
    "## GAN\n",
    "This part of the code defines the model as a whole that is used for training later on. It takes the generator and the discriminator as an input and sets all hyperparameters needed for the training. The fit method is later called on this model. It has methods to save and set the weights of both the generator and discriminator.\n",
    "\n",
    "The structure and the methods train_step and gradient_penalty were taken from: \\\n",
    "https://keras.io/examples/generative/wgan_gp/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d993e32",
   "metadata": {},
   "source": [
    "The method train_step is decorated by the function \"@tf.function\". This converts the code into an executable graph when training. This results in a longer loading time until the training starts but makes the code significantly faster when the graph is executed. To use this function only tensorflow's own data types are allowed. Within the graph no variable values can be changed. For more details refer to: https://www.tensorflow.org/guide/function#conditionals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b7bac4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T09:46:50.928801Z",
     "start_time": "2021-11-04T09:46:50.904866Z"
    }
   },
   "outputs": [],
   "source": [
    "class WGAN_GP(keras.Model):\n",
    "    '''\n",
    "    build the GAN network with wasserstein loss and gradient penalty\n",
    "    inherits from keras Model\n",
    "    discriminator: discriminator network\n",
    "    generator: generator network\n",
    "    latent_dim: latent dimension for generator input\n",
    "    n_epochs: number of epochs that it should the both networks with\n",
    "    fade_in: if both networks are fade in networks or not\n",
    "    discriminator_extra_steps: how many training steps the discriminator should be trained for every training step of the generator\n",
    "    gp_weight: the weight for the gradient penalty\n",
    "    epsilon_drift: the weight for the output drift penalty\n",
    "    '''\n",
    "    def __init__(self, discriminator, generator, latent_dim,n_epochs,fade_in,\n",
    "                 discriminator_extra_steps=3, gp_weight=10, epsilon_drift = 0.0\n",
    "                 ):\n",
    "        super(WGAN_GP, self).__init__()  # call the constructure of the parent class\n",
    "        # instantiate all class variables\n",
    "        self.discriminator = discriminator  \n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.n_epochs = n_epochs\n",
    "        self.fade_in = fade_in\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.gp_weight = gp_weight\n",
    "        self.epsilon_drift = epsilon_drift\n",
    "        \n",
    "        \n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        '''\n",
    "        compile the model, basically initializes the optimizers and the loss functions\n",
    "        '''\n",
    "        super(WGAN_GP, self).compile()  # call the parent compile function\n",
    "        self.d_optimizer = d_optimizer  # discrimintor optimizer\n",
    "        self.g_optimizer = g_optimizer # generator optimizer\n",
    "        self.d_loss_fn = d_loss_fn # discriminator loss function\n",
    "        self.g_loss_fn = g_loss_fn # generator loss function\n",
    "    \n",
    "    # forward pass of the network, needs to be defined when using subclassing with tf.function\n",
    "    def call(self, data):\n",
    "        '''\n",
    "        calculates the forward pass of the network\n",
    "        not needed because the generator and the discriminator are trained internally \n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    @tf.function # tf.function converts the decorated funciton into an executable graph (eager execution is disabled) \n",
    "    def train_step(self, real_samples):\n",
    "        '''\n",
    "        executes one training step of the GAN network this involes calculating an applying the gradients of the\n",
    "        loss functions for the discriminator and the generator w.r.t. the parameters \n",
    "        real_samples: batch of training data of real songs\n",
    "        '''\n",
    "        if isinstance(real_samples, tuple): # check for tuple data type\n",
    "            real_samples = real_samples[0] # extract the training data from the tuple\n",
    "        batch_size = tf.shape(real_samples)[0] # extract the batch size\n",
    "    \n",
    "        # train the discriminator\n",
    "    \n",
    "        for i in range(self.d_steps): # execute the number of discriminator training steps that was defined before training the generator\n",
    "            random_latent_vectors = tf.random.uniform(shape=(batch_size,latent_dim), # get the latent vector from a uniform [-1,1] distribution\n",
    "                                                  minval=-1, maxval=1)\n",
    "            with tf.GradientTape() as tape: # create a gradient tape\n",
    "                fake_samples = self.generator(  # generate fake samples with the generator\n",
    "                    random_latent_vectors, training=True)\n",
    "                fake_logits = self.discriminator(fake_samples, training=True) # get the critic score of the discriminator for the fake samples\n",
    "                real_logits = self.discriminator(real_samples, training=True) # get the critic score of the discriminator for the real samples\n",
    "         \n",
    "                d_cost = self.d_loss_fn(real_sample=real_logits,  # calculate the discriminator loss with the loss funciton passed in the compile method\n",
    "                                        fake_sample=fake_logits)\n",
    "                gp = self._gradient_penalty( # calculate the gradient penaly with the defined function\n",
    "                    batch_size, real_samples, fake_samples)\n",
    "                \n",
    "                # add the gradient penalty and the drift penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight + self.epsilon_drift *((tf.math.abs(tf.reduce_mean(fake_logits))+tf.math.abs(tf.reduce_mean(real_logits)))/2)**2\n",
    "\n",
    "            d_gradient = tape.gradient(        \n",
    "                d_loss, self.discriminator.trainable_variables) # get the gradient w.r.t the discriminator loss\n",
    "            \n",
    "            self.d_optimizer.apply_gradients(  \n",
    "                zip(d_gradient, self.discriminator.trainable_variables) # update the weights of the discriminator using the discrminator optimizer\n",
    "            )  \n",
    "            \n",
    "        # train the generator\n",
    "        \n",
    "        random_latent_vectors = tf.random.uniform(shape=(batch_size,latent_dim),\n",
    "                                                  minval=-1, maxval=1)          # get the latent vector\n",
    "       \n",
    "        with tf.GradientTape() as tape:  # create a gradient tape\n",
    "            generated_samples = self.generator(random_latent_vectors,training=True) #generate fake images using the generator\n",
    "            gen_sample_logits = self.discriminator(generated_samples,training=True) # get the discriminator critic score for fake samples\n",
    "            g_loss = self.g_loss_fn(gen_sample_logits) # calculate the generator loss\n",
    "        \n",
    "        \n",
    "        \n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables) # get the gradients w.r.t. the generator loss\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables) # update the weights of the generator using the genrator optimizer\n",
    "        )\n",
    "        return {\"d_loss\":d_loss, \"g_loss\":g_loss}  # return the losses as a dictionary\n",
    "                \n",
    "    def _gradient_penalty(self, batch_size, real_samples, fake_samples):\n",
    "        '''\n",
    "        calculates the gradient penalty to enforce the lipschitz constraint\n",
    "        batch_size: batch size of the training\n",
    "        real_samples: real audio samples\n",
    "        fake_samples: samples generated by the generator\n",
    "        '''\n",
    "\n",
    "        # get inerpolated sample, so a sample which lies between the real and the fake one\n",
    "        alpha = tf.random.normal(\n",
    "            shape=[batch_size, 1,1], mean=0.0, stddev=1.0)\n",
    "        diff = fake_samples - real_samples\n",
    "        interpolated = real_samples + alpha * diff # create the interpolated sample\n",
    "\n",
    "        with tf.GradientTape() as gp_tape: # create a gradient tape\n",
    "            gp_tape.watch(interpolated) # watch the gradient for the data interpolated\n",
    "            pred = self.discriminator(interpolated, training=True)  # get the critic score for the interpolated sample\n",
    "\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0] # get the gradients \n",
    "        # calculate gradient norm\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2]))\n",
    "        gp = tf.reduce_mean((norm-1.0)**2)  \n",
    "        return gp\n",
    "    \n",
    "    \n",
    "    def update_generator_weights(self,generator_weights):\n",
    "        '''\n",
    "        upadte the weights of the generator with some given weights\n",
    "        generator_weights: dictionary of layer names and weights create by the methond save_generator_weights\n",
    "        '''\n",
    "        for i in range(len(self.generator.layers)): # loop through all layers\n",
    "            try: self.generator.layers[i].set_weights(generator_weights[self.generator.layers[i].name]) # check if layer name matches with the dict and set weights\n",
    "            except: pass # if layer name is not found in dict then skip the layer\n",
    "    def update_discriminator_weights(self,discriminator_weights):\n",
    "        '''\n",
    "        upadte the weights of the discriminator with some given weights\n",
    "        discriminator_weights: dictionary of layer names and weights create by the methond save_discriminator_weights\n",
    "        '''\n",
    "        for i in range(len(self.discriminator.layers)): # loop through all layers\n",
    "            try: self.discriminator.layers[i].set_weights(discriminator_weights[self.discriminator.layers[i].name]) # check if layer name matches with the dict and set weights\n",
    "            except: pass # if layer name is not found in dict then skip the layer\n",
    "            \n",
    "    def save_generator_weights(self):\n",
    "        '''\n",
    "        create a dictionary with the layers names and their weights of the generator\n",
    "        '''\n",
    "        generator_weights = {} # instantate dict\n",
    "        for i in range(len(self.generator.layers)): # loop through layers\n",
    "            key = self.generator.layers[i].name # save name as key\n",
    "            value = self.generator.layers[i].get_weights() # save weights as value\n",
    "            generator_weights[key]=value # add key value pair\n",
    "        return generator_weights # return dict\n",
    "    def save_discriminator_weights(self):\n",
    "        '''\n",
    "        create a dictionary with the layers names and their weights of the discriminator\n",
    "        '''        \n",
    "        discriminator_weights = {} # instantate dict\n",
    "        for i in range(len(self.discriminator.layers)):# loop through layers\n",
    "            key = self.discriminator.layers[i].name # save name as key\n",
    "            value = self.discriminator.layers[i].get_weights() # save weights as value\n",
    "            discriminator_weights[key]=value # add key value pair\n",
    "        return discriminator_weights # return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4371f92",
   "metadata": {},
   "source": [
    "# Data Loading\n",
    "Because of the large size of the training data, not all the data is loaded into memory at once. The BatchGenerator provided the data to the model batch-wise. This is usually done by the CPU while the actually training (calculating and applying gradients) is done by a GPU. Therefore several batches can be preloaded into a queue, this is defined later in the fit method.\n",
    "\n",
    "This part of the code was inspired by: https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0ad3ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T10:37:36.111873Z",
     "start_time": "2021-11-04T10:37:36.093921Z"
    }
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(tf.keras.utils.Sequence):\n",
    "    '''\n",
    "    inherit from keras Sequence\n",
    "    creats BatchGenerator that returns a batch of training data loaded from the given folder\n",
    "    filenames: .npy file with the filenames of files in a folder that will be loaded\n",
    "    batch_size: how many files will be loaded\n",
    "    training_data_dir: folder name that lies within the TRAINING_DATA_BASE_DIR folder\n",
    "    '''\n",
    "  \n",
    "    def __init__(self, filenames, batch_size, trainig_data_dir) :\n",
    "        self.filenames = filenames\n",
    "        self.batch_size = batch_size\n",
    "        self.trainig_data_dir = trainig_data_dir\n",
    "    \n",
    "    \n",
    "    def __len__(self) :\n",
    "        '''\n",
    "        caculated the number of batches for a given batch size\n",
    "        '''\n",
    "        return (np.ceil(len(self.filenames) / float(self.batch_size))).astype(np.int)\n",
    "  \n",
    "  \n",
    "    def __getitem__(self, idx) :\n",
    "        '''\n",
    "        loads the data from the defined folder and returns a batch of data\n",
    "        '''\n",
    "        batch_x = self.filenames[idx * self.batch_size : (idx+1) * self.batch_size] # get filenames of the files for the batch\n",
    "        \n",
    "        sound_files = np.array([np.load(f\"{TRAINING_DATA_BASE_DIR}/{self.trainig_data_dir}/{file_name}\",allow_pickle=True)\n",
    "                         for file_name in batch_x]) # load the .npy files from the folder given the filename\n",
    "        sound_files = np.expand_dims(sound_files, axis=-1) # add an axis so its compatilbe with the network input sturcture\n",
    "        return sound_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf8de24",
   "metadata": {},
   "source": [
    "# Monitoring\n",
    "\n",
    "Here, the keras callback is defined. This class provides methods that are executed during the training process. It allows to output the current loss and save intermediate results. After every epoch of the full network (fade_in = False) the generator and the discriminator are saved along with a graph that shows the loss for each batch for that epoch. \n",
    "\n",
    "This part of the code was inspired by: https://www.tensorflow.org/guide/keras/custom_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551cbd78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T10:37:37.718963Z",
     "start_time": "2021-11-04T10:37:37.702904Z"
    }
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    '''\n",
    "    inherits from kreas callback\n",
    "    create a custom callback with methods that are executed after some part of the training is done\n",
    "    latent_dim: latent dimension of the generator input vector\n",
    "    sample_rate: sampling rate of the current network level\n",
    "    iteration: indicating the level of the current network (iteration 0 corresponds to level 1)\n",
    "    fade_in: whether the networsk are fade in or full networks (True/False)\n",
    "    n_epochs: number of epochs for the training iteration\n",
    "    n_batches: number of batches for the training iteration\n",
    "    '''\n",
    "    def __init__(self,latent_dim,sample_rate,iteration,fade_in,n_epochs,n_batches):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sample_rate = sample_rate\n",
    "        self.iteration = iteration\n",
    "        self.fade_in = fade_in\n",
    "        self.n_epochs = n_epochs\n",
    "        self.n_batches = n_batches\n",
    "        self.g_loss_history = []  # instantiate an empty list to save the generator loss values\n",
    "        self.d_loss_history = []  # instantiate an empty list to save the discriminator loss values\n",
    "        self.epoch = 1 # instantiate the current epoch with one\n",
    "\n",
    "        \n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        '''\n",
    "        gets executed when one training epoch has finished\n",
    "        epoch: number of current epoch\n",
    "        logs: loss dict for loss values of the generator and discriminator\n",
    "        '''\n",
    "        self.epoch = epoch\n",
    "        if not(self.fade_in): # if fade_in == False then output the loss history and save the discriminator and generator\n",
    "            g_name = f\"{MONITORING_DIR}/iteration_{self.iteration}_generator_epoch_{epoch+1}.h5\" # define gen name for current iteration and epoch\n",
    "            d_name = f\"{MONITORING_DIR}/iteration_{self.iteration}_discriminator_epoch_{epoch+1}.h5\" # define discr name for current iteration and epoch\n",
    "            self.model.generator.save(g_name) # save generator\n",
    "            self.model.discriminator.save(d_name) # save discriminator\n",
    "            \n",
    "            plt.plot(self.d_loss_history, label='d_loss') # create a plot of discriminator loss\n",
    "            plt.plot(self.g_loss_history, label='g_loss') # add the generator loss to the same plot\n",
    "            plt.legend() # add legend\n",
    "            plt.savefig(f'{MONITORING_DIR}/iteration_{self.iteration}_loss_histroy_epoch_{epoch+1}.png') # save the plot\n",
    "            plt.close() # close the plot\n",
    "            self.g_loss_history, self.d_loss_history = [],[]  #reset loss values, so we get the history only for the epoch\n",
    "        else: # if fade_in == True then increase the alpha value for the weighted sume\n",
    "            alpha_new_value = epoch/self.n_epochs # alpah is the fraction of the current epoch and the number of total training epochs\n",
    "            for layer in self.model.discriminator.layers: # loops through all layers in the discr\n",
    "                if isinstance(layer, WeightedSum): # check if its the WeightedSum layer\n",
    "                    keras.backend.set_value(layer.alpha, alpha_new_value) # adjust the alpha value\n",
    "            for layer in self.model.generator.layers: # loops through all layers in the gen\n",
    "                if isinstance(layer, WeightedSum): # check if its the WeightedSum layer\n",
    "                    keras.backend.set_value(layer.alpha, alpha_new_value) # adjust the alpha value\n",
    "        gc.collect() # automatic garbage collection to help avoid out of memory error\n",
    "        tf.keras.backend.clear_session() # clear keras backend to avoid out of memeory errr\n",
    "        \n",
    "        \n",
    "\n",
    "    def on_train_batch_end(self,batch,logs=None):\n",
    "        '''\n",
    "        gets executed after training with one batch has finished\n",
    "        batch: current batch number\n",
    "        logs: dict with loss values for gen and discr\n",
    "        '''\n",
    "        d_loss = tf.round(logs[\"d_loss\"]) # save the discr loss\n",
    "        g_loss = tf.round(logs[\"g_loss\"]) # save the gen loss\n",
    "        self.d_loss_history.append(d_loss) # add the discr loss to the list\n",
    "        self.g_loss_history.append(g_loss) # add the gen loss to the list\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "                \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7340e8e2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d310da",
   "metadata": {},
   "source": [
    "## Instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae91c13",
   "metadata": {},
   "source": [
    "Here, some hyperparameters like number of epochs, batch sizes, optimizers, loss functions and penalty weights are defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f0d0df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T10:37:38.526496Z",
     "start_time": "2021-11-04T10:37:38.511535Z"
    }
   },
   "outputs": [],
   "source": [
    "growing_iterations = 8 # define number of growing iterations\n",
    "\n",
    "\n",
    "EPOCHS_FADE_IN = [7,7,7,7,7,7,7,7,7] # set number of epochs for the fade in networks\n",
    "EPOCHS_FULL = [15,15,15,15,15,15,15,15,15] # set number of epochs for the full networks\n",
    "assert len(EPOCHS_FADE_IN) == growing_iterations+1 # make sure right number of epochs defined\n",
    "assert len(EPOCHS_FULL) == growing_iterations+1 # make sure right number of epochs defined\n",
    "\n",
    "latent_dim = 256 # set latent dimension for generator input\n",
    "\n",
    "BATCH_SIZE = [32,32,32,16,16,16,16,16,16] # set batch size\n",
    "assert len(BATCH_SIZE) == growing_iterations+1 # make sure right number of batch sizes defined\n",
    "\n",
    "discriminator_steps = 2 # set number of discriminator training steps per generator training step\n",
    "\n",
    "\n",
    "gp_weight = 10.0 # weight for gradient penalty of discriminator loss\n",
    "epsilon_drift = 0.002 # weight for drift penalty for discriminator loss\n",
    "\n",
    "# istantiate optimizers\n",
    "generator_optimizer = keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                            beta_1=0.0,\n",
    "                                            beta_2=0.9,\n",
    "                                            epsilon=10e-8\n",
    "                                            )\n",
    "discriminator_optimizer = keras.optimizers.Adam(learning_rate=0.001,\n",
    "                                               beta_1=0.0,\n",
    "                                               beta_2=0.9,\n",
    "                                               epsilon=10e-8)\n",
    "# define losses\n",
    "# gp is added later to discriminator loss\n",
    "def d_loss_fn(real_sample, fake_sample):\n",
    "    '''\n",
    "    defines the loss function of the discriminator\n",
    "    the penalties for the drift and the gradient are added later\n",
    "    real_samples: critic score for real samples\n",
    "    fake_samples: critic score for fake samples generated by the generator\n",
    "    '''\n",
    "    real_loss = tf.reduce_mean(real_sample)\n",
    "    fake_loss = tf.reduce_mean(fake_sample)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "def g_loss_fn(fake_sample):\n",
    "    '''\n",
    "    defines the loss function of the generator\n",
    "    fake_sample: critic score of the fake samples generated by the generator\n",
    "    '''\n",
    "    return -tf.reduce_mean(fake_sample)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e657e2d",
   "metadata": {},
   "source": [
    "## Growing Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4994d0",
   "metadata": {},
   "source": [
    "This part of the code executes the actual training. It iterates over the different levels of the model and trains first the fade in models and then the full models (expect for level one, there is no fade in). The discriminator and generator are saved after every epoch, therefore, the epoch which should be loaded has to be defined as well. If the training should start with an intermediate result, then the parameter \"starting_iteration\" should be set accordingly (level one corresponds to starting_iteration = 0). \"continue_with_same_level\" can be set to True if the same level should be continued training and False if the training should load the models and then start training the next level. For example when training was interrupted after epoch 8 in iteration 5 and the model should train two more epochs for iteration 5 then set the paremeters as following. starting_iteration = 5, latest_epoch_saved = 8, continue_with_same_level = True and set the number of epochs to 2 during instantiation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5481252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-04T13:12:21.387478Z",
     "start_time": "2021-11-04T13:12:17.248052Z"
    }
   },
   "outputs": [],
   "source": [
    "starting_iteration = 0  # with which iteration should the training start\n",
    "latest_epoch_saved = 0  # which epoch should be loaded (leave zero for first execution)\n",
    "continue_with_same_level = True # for an intermediate training start indicate whether to continue training with the same level\n",
    "eager_execution = False # enable eager_execution if necessary\n",
    "\n",
    "for iteration in range(starting_iteration,growing_iterations+1):\n",
    "    epochs = EPOCHS_FULL # set epochs to EPOCHS_FULL\n",
    "    n_epochs = epochs[iteration] # extracht number of epochs\n",
    "\n",
    "    batch_size = BATCH_SIZE[iteration]   # define the batch size for the iteration\n",
    "    sample_rate = FINISH_SAMPLE_RATE/2**(growing_iterations-iteration) # define the sample rate for the resolution\n",
    "    training_data_dir = f\"sr_{int(sample_rate)}\"  # define the training directory based on the sample rate\n",
    "    filenames = np.load(f\"{FILENAME_DIR}/filenames.npy\",allow_pickle=True) # get the filenames out of the regarding file\n",
    "    \n",
    "    training_data_generator = BatchGenerator(filenames,batch_size,training_data_dir) # define the Iterator Object\n",
    "    n_batches = len(training_data_generator) # get the number of batches\n",
    "\n",
    "    for fade_in in [True,False]: # execute fade in training and then full training\n",
    "        print(f\"Start Iteration {iteration}, fade_in:{fade_in}\")\n",
    "        \n",
    "        \n",
    "        if fade_in == False: # for fade_in get the regarding epoch numbers\n",
    "            epochs = EPOCHS_FADE_IN\n",
    "            n_epochs = epochs[iteration]    \n",
    "        callbacks = GANMonitor(latent_dim=latent_dim,sample_rate=sample_rate,iteration=iteration, \n",
    "                               fade_in = fade_in, n_epochs=n_epochs,n_batches=n_batches) #instantiate callback Object\n",
    "        \n",
    "        # check if training starts with and interim model and load weights accordlingly\n",
    "        if iteration == starting_iteration and fade_in == True: continue # skip fade_in for the starting iteration\n",
    "        if iteration == starting_iteration and fade_in == False and starting_iteration != 0 and latest_epoch_saved != 0: # if training starts with a later level load models\n",
    "            # load gen\n",
    "            generator = load_model(f'./{MONITORING_DIR}/iteration_{starting_iteration}_generator_epoch_{latest_epoch_saved}.h5', custom_objects={\n",
    "                                                                                'PixelNormalization': PixelNormalization,\n",
    "                                                                                'WeightedSum': WeightedSum,\n",
    "                                                                                'Conv1DEQ':Conv1DEQ_load})\n",
    "            # load discr\n",
    "            discriminator = load_model(f'./{MONITORING_DIR}/iteration_{starting_iteration}_discriminator_epoch_{latest_epoch_saved}.h5', custom_objects={\n",
    "                                                                                'MinibatchStdev': MinibatchStdev,\n",
    "                                                                                'WeightedSum': WeightedSum,\n",
    "                                                                                'Conv1DEQ':Conv1DEQ_load})\n",
    "            # instantiate gan\n",
    "            wgan_gp = WGAN_GP(discriminator, generator, latent_dim,n_epochs=n_epochs,fade_in=fade_in,\n",
    "                         discriminator_extra_steps=discriminator_steps, gp_weight=gp_weight, epsilon_drift = epsilon_drift)\n",
    "            # save weights\n",
    "            discriminator_weights = wgan_gp.save_discriminator_weights()\n",
    "            generator_weights = wgan_gp.save_generator_weights()\n",
    "            \n",
    "            \n",
    "            if continue_with_same_level == False: continue # skip training for this iteration because the models are already trained and the weights saved\n",
    "\n",
    "\n",
    "        # start training    \n",
    "        generator = create_generator(latent_dim,iteration,fade_in=fade_in) # instantiate generator\n",
    "        discriminator = create_discriminator(iteration,fade_in=fade_in) # instatiate discriminator\n",
    "        wgan_gp = WGAN_GP(discriminator, generator, latent_dim,n_epochs=n_epochs,fade_in=fade_in,\n",
    "                         discriminator_extra_steps=discriminator_steps, gp_weight=gp_weight)  # instantiate GAN  \n",
    "        # if iteration is greater than zero update the weights of the network with the saved values\n",
    "        if iteration > 0:\n",
    "            wgan_gp.update_generator_weights(generator_weights)\n",
    "            wgan_gp.update_discriminator_weights(discriminator_weights)\n",
    "        # compile the GAN    \n",
    "        wgan_gp.compile(\n",
    "            d_optimizer=discriminator_optimizer,\n",
    "            g_optimizer=generator_optimizer,\n",
    "            g_loss_fn=g_loss_fn,\n",
    "            d_loss_fn=d_loss_fn,\n",
    "        )\n",
    "        \n",
    "        wgan_gp.run_eagerly = eager_execution  # enable eager execution if set to True\n",
    "        \n",
    "        # Fit teh GAN: uses the keras interanl Fit function with a Generator and a custom callback \n",
    "        wgan_gp.fit(training_data_generator,batch_size=batch_size, epochs=n_epochs,callbacks=[callbacks],\n",
    "                 verbose=1,max_queue_size=8,workers=8)\n",
    "        \n",
    "        discriminator_weights = wgan_gp.save_discriminator_weights() # save discr weights for next iteration\n",
    "        generator_weights = wgan_gp.save_generator_weights() # save gen weights for next iteration\n",
    "        del generator # delete generator object\n",
    "        del discriminator # delete discriminator object\n",
    "        del wgan_gp # delete GAN object\n",
    "        gc.collect() # python garbage collection to avoid OOM\n",
    "        tf.keras.backend.clear_session() # keras clear backend session OOM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.54499999999999,
   "position": {
    "height": "40px",
    "left": "1126.36px",
    "right": "20px",
    "top": "120px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
